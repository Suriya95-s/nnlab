{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2n4JVbv5VMc4b3Xsd+sob"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mSFHkMOM94ha","executionInfo":{"status":"ok","timestamp":1722902212247,"user_tz":-330,"elapsed":10394,"user":{"displayName":"Suriya S","userId":"16319069296289037144"}},"outputId":"c2ad19a8-cba8-46f0-ea7e-828b910fd323"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n","Epoch 0, D Loss: 0.7122501134872437, G Loss: [array(0.7058938, dtype=float32), array(0.7058938, dtype=float32), array(0.328125, dtype=float32)]\n","Discriminator Accuracy: 0.3281\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n","\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","\n","\n","(X_train, _), _ = keras.datasets.mnist.load_data()\n","X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n","X_train = np.expand_dims(X_train, axis=-1)\n","\n","\n","generator = keras.Sequential([\n","    keras.layers.Dense(7 * 7 * 128, input_shape=(100,)),\n","    keras.layers.Reshape((7, 7, 128)),\n","    keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding='same'),\n","    keras.layers.LeakyReLU(alpha=0.2),\n","    keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding='same', activation='tanh')\n","])\n","\n","\n","discriminator = keras.Sequential([\n","    keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=(28, 28, 1)),\n","    keras.layers.LeakyReLU(alpha=0.2),\n","    keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same'),\n","    keras.layers.LeakyReLU(alpha=0.2),\n","    keras.layers.Flatten(),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])\n","discriminator.compile(loss='binary_crossentropy',\n","                      optimizer=keras.optimizers.Adam(learning_rate=0.0002),\n","                      metrics=['accuracy'])\n","discriminator.trainable = False\n","\n","\n","gan_input = keras.Input(shape=(100,))\n","generated_image = generator(gan_input)\n","gan_output = discriminator(generated_image)\n","gan = keras.Model(gan_input, gan_output)\n","gan.compile(loss='binary_crossentropy',\n","            optimizer=keras.optimizers.Adam(learning_rate=0.0002))\n","\n","batch_size, epochs, sample_interval = 64, 10, 1000\n","\n","for epoch in range(epochs):\n","    idx = np.random.randint(0, X_train.shape[0], batch_size)\n","    real_images = X_train[idx]\n","    noise = np.random.normal(0, 1, (batch_size, 100))\n","    fake_images = generator.predict(noise)\n","\n","    d_loss_real = discriminator.train_on_batch(real_images, np.ones((batch_size, 1)))\n","    d_loss_fake = discriminator.train_on_batch(fake_images, np.zeros((batch_size, 1)))\n","    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","\n","    noise = np.random.normal(0, 1, (batch_size, 100))\n","    g_loss = gan.train_on_batch(noise, np.ones((batch_size, 1)))\n","\n","    if epoch % sample_interval == 0:\n","        print(f'Epoch {epoch}, D Loss: {d_loss[0]}, G Loss: {g_loss}')\n","        _, accuracy = discriminator.evaluate(\n","            np.concatenate([real_images, fake_images]),\n","            np.concatenate([np.ones((batch_size, 1)), np.zeros((batch_size, 1))]), verbose=0\n","        )\n","        print(f\"Discriminator Accuracy: {accuracy:.4f}\")\n"]}]}